# server.py
from flask import Flask, request, jsonify
import joblib
import zipfile
import os

app = Flask(__name__)

# Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø¶ØºÙˆØ·
ZIP_PATH = "rf_smote_rf_model.zip"
MODEL_PATH = "rf_smote_rf_model.pkl"
SCALER_PATH = "scaler.pkl"

# ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¥Ø°Ø§ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©
if not os.path.exists(MODEL_PATH) or not os.path.exists(SCALER_PATH):
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(".")  # ÙŠÙÙƒ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ scaler
model = joblib.load(MODEL_PATH)
scaler = joblib.load(SCALER_PATH)

# âœ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ÙŠØ©
@app.route("/", methods=["GET"])
def home():
    return jsonify({
        "message": "ğŸš¦ Traffic ML API is running!",
        "endpoints": {
            "/": "ÙˆØ§Ø¬Ù‡Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ©",
            "/predict": "Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© (srcPort, dstPort, protocol, size) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹"
        },
        "example": {
            "srcPort": 12345,
            "dstPort": 80,
            "protocol": 6,
            "size": 512
        }
    })

# âœ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙˆÙ‚Ø¹
@app.route("/predict", methods=["POST"])
def predict():
    data = request.json

    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù€ features
    features = [[
        data.get("srcPort", 0),
        data.get("dstPort", 0),
        data.get("protocol", 0),
        data.get("size", 0)
    ]]

    # ØªØ­ÙˆÙŠÙ„ ÙˆØªÙˆÙ‚Ø¹
    X = scaler.transform(features)
    pred = model.predict(X)[0]

    # 0 = Ø¹Ø§Ø¯ÙŠ â†’ Ø£Ø®Ø¶Ø±ØŒ 1 = Ø®Ø·Ø± â†’ Ø£Ø­Ù…Ø±
    response = {
        "label": int(pred),
        "isValid": (pred == 0),   # âœ… Boolean
        "color": "green" if pred == 0 else "red"  # âœ… String
    }

    return jsonify(response)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
